<!DOCTYPE html>
<html>
<head><meta name="generator" content="Hexo 3.9.0">
  <meta charset="utf-8">
  

  
  <title>动手学 pytorch-task01-02 | yuiのブログ</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="tensor 基础创建tensor创建特定tensor： *torch.from_numpy(a) *numpy向量转tensor  torch.tensor([2,2])  torch.FloatTensor([2,2.])  torch.FloatTensor([[1,2],[3,4]])列表转tensor  torch.empty(size)返回形状为size的空tensor  torch.">
<meta property="og:type" content="article">
<meta property="og:title" content="动手学 pytorch-task01-02">
<meta property="og:url" content="http://yoursite.com/2020/02/13/动手学-pytorch-1/index.html">
<meta property="og:site_name" content="yuiのブログ">
<meta property="og:description" content="tensor 基础创建tensor创建特定tensor： *torch.from_numpy(a) *numpy向量转tensor  torch.tensor([2,2])  torch.FloatTensor([2,2.])  torch.FloatTensor([[1,2],[3,4]])列表转tensor  torch.empty(size)返回形状为size的空tensor  torch.">
<meta property="og:locale" content="en">
<meta property="og:updated_time" content="2020-02-14T06:32:31.441Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="动手学 pytorch-task01-02">
<meta name="twitter:description" content="tensor 基础创建tensor创建特定tensor： *torch.from_numpy(a) *numpy向量转tensor  torch.tensor([2,2])  torch.FloatTensor([2,2.])  torch.FloatTensor([[1,2],[3,4]])列表转tensor  torch.empty(size)返回形状为size的空tensor  torch.">
  
    <link rel="alternate" href="/atom.xml" title="yuiのブログ" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  <link rel="stylesheet" href="/css/style.css">
</head>
</html>
<body>
  <div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">yuiのブログ</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="Search"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="http://yoursite.com"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-动手学-pytorch-1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2020/02/13/动手学-pytorch-1/" class="article-date">
  <time datetime="2020-02-13T12:43:39.000Z" itemprop="datePublished">2020-02-13</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      动手学 pytorch-task01-02
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="tensor-基础"><a href="#tensor-基础" class="headerlink" title="tensor 基础"></a>tensor 基础</h2><h3 id="创建tensor"><a href="#创建tensor" class="headerlink" title="创建tensor"></a>创建tensor</h3><h4 id="创建特定tensor："><a href="#创建特定tensor：" class="headerlink" title="创建特定tensor："></a>创建特定tensor：</h4><ul>
<li><p>*<em><code>torch.from_numpy(a)</code> *</em><br>numpy向量转tensor</p>
</li>
<li><p><strong><code>torch.tensor([2,2])</code></strong></p>
</li>
<li><p><strong><code>torch.FloatTensor([2,2.])</code></strong></p>
</li>
<li><p><strong><code>torch.FloatTensor([[1,2],[3,4]])</code></strong><br>列表转tensor</p>
</li>
<li><p><strong><code>torch.empty(size)</code></strong><br>返回形状为size的空tensor</p>
</li>
<li><p><strong><code>torch.zeros(size)</code></strong><br>全部是0的tensor</p>
</li>
<li><p><strong><code>torch.zeros_like(input)</code></strong><br>返回跟input的tensor一个size的全零tensor</p>
</li>
<li><p><strong><code>torch.ones(size)</code></strong><br>全部是1的tensor</p>
</li>
<li><p><strong><code>torch.ones_like(input)</code></strong><br>返回跟input的tensor一个size的全一tensor</p>
</li>
<li><p><strong><code>torch.arange(start=0, end, step=1)</code></strong><br>返回一个从start到end的序列，可以只输入一个end参数，就跟python的range()一样了。实际上PyTorch也有range()，但是这个要被废掉了，替换成arange了</p>
</li>
<li><p><strong><code>torch.full(size, fill_value)</code></strong><br>这个有时候比较方便，把fill_value这个数字变成size形状的张量</p>
</li>
</ul>
<h4 id="随机采样生成："><a href="#随机采样生成：" class="headerlink" title="随机采样生成："></a>随机采样生成：</h4><ul>
<li><strong><code>torch.round(size)</code></strong><br> 在[0,1]内的均匀分布的随机数</li>
<li><strong><code>torch.rand_like(input)</code></strong><br>返回跟input的tensor一样size的0-1随机数</li>
<li><strong><code>torch.randn(size)</code></strong><br>返回标准正太分布N(0,1)的随机数</li>
<li><strong><code>torch.normal(mean, std, out=None)</code></strong><br>正态分布。这里注意，mean和std都是tensor，返回的形状由mean和std的形状决定，一般要求两者形状一样。如果，mean缺失，则默认为均值0，如果std缺失，则默认标准差为1.</li>
</ul>
<h3 id="torch-基本操作"><a href="#torch-基本操作" class="headerlink" title="torch 基本操作"></a>torch 基本操作</h3><h4 id="索引："><a href="#索引：" class="headerlink" title="索引："></a>索引：</h4><ul>
<li><p><strong><code>index_select</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=torch.rand(4,3,28,28) ###4张图片</span><br><span class="line">x.index_select(0,torch.tensor([0,1,2]))#第一个参数为轴，第二个参数为tensor类型的索引</span><br><span class="line">x.index_select(0,torch.arange(3))#效果同上句</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>torch.masked_select</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=torch.rand(3,4)</span><br><span class="line">mask=x.ge(0.5)#会把x中大于0.5的置为一，其他置为0，类似于阈值化操作。</span><br><span class="line">y=torch.masked_select(x,mask)#将mask中值为1的元素取出来，比如mask有3个位置值为1</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>torch.take</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">x=torch.tensor([[1,2,3],[4,5,6]])</span><br><span class="line">torch.take(x,torch.tensor([0,2,6]))</span><br><span class="line">#则最后结果为tensor([1,3,6]),也就是说会先将tensor压缩成一维向量，再按照索引取元素。</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="切片-start-end-step"><a href="#切片-start-end-step" class="headerlink" title="切片:(start : end : step)"></a>切片:(start : end : step)</h4><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">img=torch.rand(4,3,28,28)#4张图片</span><br><span class="line"></span><br><span class="line">img[1]#获取第二张图片</span><br><span class="line">img[0,0].shape#获取第一张图片的第一个通道的图片形状</span><br><span class="line">img[0,0,2,4]#返回像素灰度值标量</span><br><span class="line"></span><br><span class="line">img[:2]#获得img[0]和img[1]</span><br><span class="line">#img[:2,:1]==img[:2,:1,:,:]</span><br><span class="line">img[2:]#获得img[2],img[3],img[4]三张图片</span><br><span class="line">img[-1:]#获得img[4]</span><br><span class="line">img[:,:,::2,::2]#对图片进行隔行（列）采样</span><br><span class="line"></span><br><span class="line">#还有一种索引中的...操作，有自动填充的功能,一般用于维数很多时使用。</span><br><span class="line">img[0,...]</span><br><span class="line">#img.shape的结果是torch.Size([4,28,28]),这是和img[0,:]或者img[0]是一样的。</span><br><span class="line">img[0,...,0:28:2]</span><br><span class="line">#此时由于写了最右边的索引，中间的...等价于:,:，即img[0,:,:,0:28:2]</span><br></pre></td></tr></table></figure></code></pre><h4 id="维度变换（view-reshape-squeeze-transpose-expand-permute）"><a href="#维度变换（view-reshape-squeeze-transpose-expand-permute）" class="headerlink" title="维度变换（view/reshape/squeeze/transpose/expand/permute）:"></a>维度变换（view/reshape/squeeze/transpose/expand/permute）:</h4><ul>
<li><p><strong><code>view()和reshape效果一样,都是改变shape</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">a = torch.rand(4, 1, 28, 28)</span><br><span class="line">print(a.shape)</span><br><span class="line">print(a.reshape(4 * 1, 28, 28).shape)</span><br><span class="line">print(a.reshape(4, 1 * 28 * 28).shape)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">torch.Size([4, 1, 28, 28])</span><br><span class="line">torch.Size([4, 28, 28])</span><br><span class="line">torch.Size([4, 784])</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>增加维度</code></strong><br>正的索引是在那个维度原本的位置前面插入这个新增加的维度，负的索引是在那个位置之后插入</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">print(a.shape)</span><br><span class="line">print(a.unsqueeze(0).shape)  # 在0号维度位置插入一个维度</span><br><span class="line">print(a.unsqueeze(-1).shape)  # 在最后插入一个维度</span><br><span class="line">print(a.unsqueeze(3).shape)  # 在3号维度位置插入一个维度</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">torch.Size([4, 1, 28, 28])</span><br><span class="line">torch.Size([1, 4, 1, 28, 28])</span><br><span class="line">torch.Size([4, 1, 28, 28, 1])</span><br><span class="line">torch.Size([4, 1, 28, 1, 28])</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>删减维度</code></strong><br>删减维度实际上是一个压榨的过程，直观地看是把那些多余的[]给去掉，也就是只是去删除那些size=1的维度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor(1, 4, 1, 9)</span><br><span class="line">print(a.shape)</span><br><span class="line">print(a.squeeze().shape) # 能删除的都删除掉</span><br><span class="line">print(a.squeeze(0).shape) # 尝试删除0号维度,ok</span><br><span class="line">print(a.squeeze(2).shape) # 尝试删除2号维度,ok</span><br><span class="line">print(a.squeeze(3).shape) # 尝试删除3号维度,3号维度是9不是1,删除失败</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">torch.Size([1, 4, 1, 9])</span><br><span class="line">torch.Size([4, 9])</span><br><span class="line">torch.Size([4, 1, 9])</span><br><span class="line">torch.Size([1, 4, 9])</span><br><span class="line">torch.Size([1, 4, 1, 9])</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>维度扩展(expand)</code></strong><br>expand就是在某个size=1的维度上改变size，改成更大的一个大小，实际就是在每个size=1的维度上的标量的广播操作</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">b = torch.rand(32)</span><br><span class="line">f = torch.rand(4, 32, 14, 14)</span><br><span class="line"></span><br><span class="line"># 想要把b加到f上面去</span><br><span class="line"></span><br><span class="line"># 先进行维度增加</span><br><span class="line">b = b.unsqueeze(1).unsqueeze(2).unsqueeze(0)</span><br><span class="line">print(b.shape)</span><br><span class="line"></span><br><span class="line"># 再进行维度扩展</span><br><span class="line">b = b.expand(4, -1, 14, 14)  # -1表示这个维度保持不变,这里写32也可以</span><br><span class="line">print(b.shape)</span><br><span class="line"></span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">torch.Size([1, 32, 1, 1])</span><br><span class="line">torch.Size([4, 32, 14, 14])</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>维度重复(repeat)</code></strong><br>repeat就是将每个位置的维度都重复至指定的次数，以形成新的Tensor。repeat会重新申请内存空间</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">b = torch.rand(1,32,1,1)</span><br><span class="line">print(b.shape)</span><br><span class="line"></span><br><span class="line"># 维度重复,32这里不想进行重复,所以就相当于&quot;重复至1次&quot;</span><br><span class="line">b = b.repeat(4, 1, 14, 14)</span><br><span class="line">print(b.shape)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">torch.Size([1, 32, 1, 1])</span><br><span class="line">torch.Size([4, 32, 14, 14])</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong><code>转置</code></strong><br>只适用于dim=2的Tensor</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">c = torch.Tensor(2, 4)</span><br><span class="line">print(c.t().shape)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">torch.Size([4, 2])</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>维度交换transpose</code></strong><br>注意这种交换使得存储不再连续，再执行一些reshape的操作肯定是执行不了的，所以要调用一下contiguous()使其变成连续的维度</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">d = torch.Tensor(6, 3, 1, 2)</span><br><span class="line">print(d.transpose(1, 3).contiguous().shape)  # 1号维度和3号维度交换</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">torch.Size([6, 2, 1, 3])</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>下面这个例子比较一下每个位置上的元素都是一致的，来验证一下这个交换-&gt;压缩shape-&gt;展开shape-&gt;交换回去是没有问题的</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">e = torch.rand(4, 3, 6, 7)</span><br><span class="line">e2 = e.transpose(1, 3).contiguous().reshape(4, 7 * 6 * 3).reshape(4, 7, 6, 3).transpose(1, 3)</span><br><span class="line">print(e2.shape)</span><br><span class="line"># 比较下两个Tensor所有位置上的元素是否都相等</span><br><span class="line">print(torch.all(torch.eq(e, e2)))</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">torch.Size([4, 3, 6, 7])</span><br><span class="line">tensor(1, dtype=torch.uint8)</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>

<ul>
<li><strong><code>permute</code></strong><br>如果四个维度表示上节的[batch,channel,h,w][batch,channel,h,w]      [batch,channel,h,w][batch,channel,h,w]，如果想把channelchannel      channelchannel放到最后去，形成[batch,h,w,channel][batch,h,w,channel]      [batch,h,w,channel][batch,h,w,channel]，那么如果使用前面的维度交换，至少要交换两次（先13交换再12交换）。而使用permute可以直接指定维度新的所处位置，方便很多<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">h = torch.rand(4, 3, 6, 7)</span><br><span class="line">print(h.permute(0, 2, 3, 1).shape)</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">torch.Size([4, 6, 7, 3])</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h4 id="tensor的拼接与拆分"><a href="#tensor的拼接与拆分" class="headerlink" title="tensor的拼接与拆分"></a>tensor的拼接与拆分</h4><p>   (cat/stack/spilit/chunk)<br>   <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">  #cat拼接</span><br><span class="line">a=torch.rand(4,3,18,18)</span><br><span class="line">b=torch.rand(5,3,18,18)</span><br><span class="line">c=torch.rand(4,1,18,18)</span><br><span class="line">d=a.copy()</span><br><span class="line"></span><br><span class="line">torch.cat([a,b],dim=0)#拼接得到(9,3,18,18)的数据</span><br><span class="line">#若为2维数据，dim=0则是竖向拼接，dim=0就是横向拼接。dim所指维度可以不同，但其他维度形状必须一致</span><br><span class="line">torch.cat([a,c],dim=1)#就会得到(4,4,18,18)的数据。</span><br><span class="line">#stack增维度拼接</span><br><span class="line">torch.stack([a,b],dim=0)#得到形状为(2,4,3,18,18)。使用时列表内对象的形状需要一致。</span><br><span class="line">#split拆分</span><br><span class="line"></span><br><span class="line">#根据欲拆分长度：</span><br><span class="line">a1,a2=a.split(2,dim=0)#拆分长度为2.对第0维按照2个一份进行拆分。拆分获得两个形状为(2,3,18,18)的张量。</span><br><span class="line">a1,a2=a.split([3,1],dim=0)#不同长度拆分。获得(3,3,18,18)和(1,3,18,18)两个形状的张量。</span><br><span class="line">#chunk拆分</span><br><span class="line"></span><br><span class="line">#根据欲拆分数量</span><br><span class="line">a1,a2,a3,a4=a.chunk(4,dim=0)#将张量依第0维拆分成4个(1,3,18,18)的张量。等效于a.split(1,dim=0)</span><br></pre></td></tr></table></figure></p>
<h4 id="tensor的运算"><a href="#tensor的运算" class="headerlink" title="tensor的运算"></a>tensor的运算</h4><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"># +等价于torch.add()</span><br><span class="line"></span><br><span class="line">#乘法</span><br><span class="line">torch.mm(a, b) ##mm只能进行矩阵乘法,不可以是tensor,也就是输入的两个tensor维度只能是 (n×m) 和 (m×p)</span><br><span class="line">torch.mul(a, b) ##是矩阵a和b对应位相乘，a和b的维度必须相等，比如a的维度是(1, 2)，b的维度是(1, 2)，返回的仍是(1, 2)的矩阵</span><br><span class="line">torch.matmul(a,b) ##可以进行张量乘法, 输入可以是高维</span><br><span class="line">torch.bmm(a,b) ## 是两个三维张量相乘, 两个输入tensor维度是 (b×n×m) 和 (b×m×p) , 第一维b代表batch size，输出为 (b×n×p)</span><br><span class="line"></span><br><span class="line">#平方</span><br><span class="line">a=torch.full([2,2],2)#创建一个(2,2)的全2矩阵</span><br><span class="line">a.pow(2)#a的每个元素都平方</span><br><span class="line">a**2#等价于上一句</span><br><span class="line"></span><br><span class="line">#开方</span><br><span class="line">a.sqrt()#平方根</span><br><span class="line">a**0.5#等价于上一句</span><br><span class="line"></span><br><span class="line">#exp,log</span><br><span class="line">a.torch.exp(torch.ones(2,2))</span><br><span class="line">torch.log(a)</span><br><span class="line"></span><br><span class="line">#近似</span><br><span class="line">#floor()向下取整，ceil()向上取整，round()四舍五入。</span><br><span class="line"></span><br><span class="line">#取整取小数</span><br><span class="line">#trunc()取整，frac()取小数</span><br><span class="line"></span><br><span class="line">#clamp取范围</span><br><span class="line">a=torch.tensor([[3,5],[6,8]])</span><br><span class="line">a.clamp(6)#得到[[6,6],[6,8]],小于6的都变为6</span><br><span class="line">a.clamp(5,6)#得到[[5,5],[6,6]],小于下限变为下限，大于上限变为上限。</span><br></pre></td></tr></table></figure></code></pre><h4 id="tensor的统计属性"><a href="#tensor的统计属性" class="headerlink" title="tensor的统计属性"></a>tensor的统计属性</h4><pre><code><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line">#范数</span><br><span class="line">#求多少p范数只需要在norm(p)的参数中修改p即可</span><br><span class="line">a.norm(1)#求a的一范数，范数也可以加dim=</span><br><span class="line"></span><br><span class="line">#求最大值和最小值与其相关的索引</span><br><span class="line">a.min()</span><br><span class="line">a.max()</span><br><span class="line">a.argmax()#会得到索引值，返回的永远是一个标量，多维张量会先拉成向量再求得其索引。拉伸的过程为每一行加起来变成一整行，而不是matlab中的列拉成一整列。</span><br><span class="line">a.argmin()</span><br><span class="line">a.argmax(dim=1)#如果不想获取拉伸后的索引值就需要在指定维度上进行argmax，比如如果a为(2，2)的矩阵，那么这句话的结果就可能是[1,1],表示第一行第一个在此行最大，第二行第一个在此行最大。</span><br><span class="line"></span><br><span class="line">#累加总和</span><br><span class="line">a.sum()</span><br><span class="line"></span><br><span class="line">#累乘综合</span><br><span class="line">a.prod()</span><br><span class="line"></span><br><span class="line">#dim,keepdim</span><br><span class="line">#假设a的形状为(4,10)</span><br><span class="line">a.max(dim=1)#结果会得到一个(4)的张量，表示4个样本中每个样本10个特征的最大值组成的张量。(max换成argmax也是同理)。</span><br><span class="line">a.max(dim=1,keepdim=True)#同时返回a.argmax(dim=1)得到的结果，以保持维度数目和原来一致。</span><br><span class="line"></span><br><span class="line">#top-k,k-th</span><br><span class="line">a.topk(5)#返回张量a前5个最大值组成的向量</span><br><span class="line">a.topk(5,dim=1,largest=False)#关闭largest求最小的5个</span><br><span class="line">a.kthvalue(8,dim=1)#返回第八小的值</span><br><span class="line"></span><br><span class="line">#比较操作</span><br><span class="line">#都是进行element-wise操作</span><br><span class="line">torch.eq(a,b)#返回的是张量</span><br><span class="line">torch.equal(a,b)#返回的是True或者False</span><br></pre></td></tr></table></figure></code></pre><h4 id="Tensor的高阶操作"><a href="#Tensor的高阶操作" class="headerlink" title="Tensor的高阶操作"></a>Tensor的高阶操作</h4><ul>
<li><p><strong><code>where</code></strong><br>用C=torch.where(condition,A,B)其中A,B,C,condition是shape相同的Tensor，C中的某些元素来自A，某些元素来自B，这由condition中相应位置的元素是1还是0来决定</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">cond = torch.tensor([[0.6, 0.1], [0.2, 0.7]])</span><br><span class="line">a = torch.tensor([[1, 2], [3, 4]])</span><br><span class="line">b = torch.tensor([[4, 5], [6, 7]])</span><br><span class="line">c = torch.where(cond &gt; 0.5, a, b) ##条件满足取a,条件不满足取b</span><br><span class="line">print(c) </span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">tensor([[1, 5],</span><br><span class="line">       [6, 4]])</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>gather</code></strong><br>torch.gather(input, dim, index, out=None)<br>沿给定轴dim，将输入索引张量index指定位置的值进行聚合<br> 对一个3维张量，输出可以定义为：</p>
<pre><code>**`out[i][j][k] = tensor[index[i][j][k]][j][k] # dim=0`**
**` out[i][j][k] = tensor[i][index[i][j][k]][k] # dim=1`**
**`out[i][j][k] = tensor[i][j][index[i][j][k]] # dim=2`**</code></pre><p>dim=1,按行操作，那么是列索引<br>dim=2,按列操作，那么是行索引</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">a = torch.Tensor([[1,2],[3,4]])</span><br><span class="line">b = torch.gather(a,1,torch.LongTensor([[0,0],[1,0]]))  </span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">tensor([[1., 1.],  </span><br><span class="line">		[4., 3.]])</span><br><span class="line">[1,2]取[0,0]-&gt;[1,1],[3,4]取[1,0]即[4,3]</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">b = torch.gather(a,2,torch.LongTensor([[1,1],[1,0]]))</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">[1,1]取[3,4],[1,0]取[3,2]</span><br><span class="line">tensor([[3., 4.],</span><br><span class="line">       [3., 2.]])</span><br><span class="line">&apos;&apos;&apos;</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h4 id="pytorch自动求导autograd"><a href="#pytorch自动求导autograd" class="headerlink" title="pytorch自动求导autograd:"></a>pytorch自动求导autograd:</h4><pre><code>autograd包是PyTorch中所有神经网络的核心。它为Tensors上的所有操作提供自动微分。它是一个自定义的框架，这意味着以代码运行方式定义后向传播，并且每次迭代都可以不同。</code></pre><ul>
<li><p><strong><code>Tensor类</code></strong><br>torch.Tensor是包的核心类。如果将其属性.requires_grad设置为Ture，则会开始跟踪针对tensor的所有操作。完成计算后，可以调用.backward()来自动计算所有梯度。该张量的梯度将累积到.grad属性中。<br>.detach()：停止tensor历史记录的跟踪，它将其与计算历史记录分离，并防止将来的计算被跟踪。<br>要停止跟踪历史记录（和使用内存），我们还可以将代码块使用with torch.no_grad():包装起来，这在评估模型时特别有用，因为模型在训练阶段具有requires_grad = True的可训练参数有利于调参，但在评估阶段我们不再需要梯度。</p>
</li>
<li><p><strong><code>function类</code></strong><br>Function类也是autograd一个非常重要的类，Tensor 和 Function 互相连接并构建一个非循环图，它保存整个完整的计算过程的历史信息。每个张量都有一个 .grad_fn 属性保存着创建了张量的 Function 的引用，（如果用户自己创建张量，则grad_fn 是 None ）。<br>如果你想计算导数，你可以调用 Tensor.backward()。如果 Tensor 是标量（即它包含一个元素数据），则不需要指定任何参数backward()，但是如果它有更多元素，则需要指定一个gradient 参数来指定张量的形状。</p>
</li>
</ul>
<h4 id="pytorch流程"><a href="#pytorch流程" class="headerlink" title="pytorch流程"></a>pytorch流程</h4><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">1: 准备数据(注意数据格式不同）</span><br><span class="line">2: 定义网络结构model</span><br><span class="line">3: 定义损失函数</span><br><span class="line">4: 定义优化算法 optimizer</span><br><span class="line">5: 训练-pytorch</span><br><span class="line">迭代训练：</span><br><span class="line">	5.1:准备好tensor形式的输入数据和标签(可选)</span><br><span class="line">	5.2:前向传播计算网络输出output和计算损失函数loss</span><br><span class="line">	5.3:反向传播更新参数</span><br><span class="line">		5.3.1:将上次迭代计算的梯度值清0</span><br><span class="line">			optimizer.zero_grad()</span><br><span class="line">		5.3.2:反向传播，计算梯度值</span><br><span class="line">			loss.backward()</span><br><span class="line">		5.3.3:更新权值参数</span><br><span class="line">			optimizer.step()</span><br><span class="line">6: 在测试集上测试-pytorch</span><br><span class="line">    遍历测试集，自定义metric</span><br><span class="line">7: 保存网络（可选） 具体实现参考上面代码</span><br></pre></td></tr></table></figure>

<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">损失函数：均方误差</span><br><span class="line">优化函数：（小批量）随机梯度下降</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong><code>从零开始的实现核心代码</code></strong></p>
<ul>
<li><p><strong><code>1.数据读取生成器</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">def data_iter(batch_size,features,labels):</span><br><span class="line">	num_examples = len(features)</span><br><span class="line">	indices = list(range(num_examples))</span><br><span class="line">	random.shuffle(indices)</span><br><span class="line">	for i in range(0,num_examples,batch_size):</span><br><span class="line">		j = torch.LongTensor(indices[i:min(i+batch_size,num_examples)])</span><br><span class="line">		yield features.index_select(0,j),labels.index_select(0,j)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>2.模型初始化参数</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">w = torch.tensor(np.random.normal(0,0.01,(num_inputs,1)),dtype=torch.float32)</span><br><span class="line">b = torch.zeros(1,dtype=torch.float32)</span><br><span class="line"></span><br><span class="line">w.requires_grad_(requires_grad =True)</span><br><span class="line">b.requires_grad_(requires_grad =True)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>3.定义模型和损失函数、优化函数</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def linreg(X,w,b):</span><br><span class="line">	return torch.mm(X,w)+b</span><br><span class="line">	</span><br><span class="line">def squared_loss(y_hat,y):</span><br><span class="line">	return (y_hat-y.view(-1,1))**2/2</span><br><span class="line">	</span><br><span class="line">def sgd(params,lr,batch_size):</span><br><span class="line">	for param in params:</span><br><span class="line">		param.data -=lr*param.grad / batch_size</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>4.模型训练</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">for epoch in range(num_epochs):</span><br><span class="line">	for X,y in data_iter(batch_size,features,labels):</span><br><span class="line">		l = loss(net(X,w,b),y).sum()</span><br><span class="line">		l.backward() ##反向传播</span><br><span class="line">		</span><br><span class="line">		sgd([w,b],lr,batch_size) ##参数优化</span><br><span class="line">		w.grad.data.zero_()</span><br><span class="line">		b.grad.data.zero_()</span><br><span class="line">	train_l =loss(net(features,w,b),labels)</span><br><span class="line">	print(&apos;epoch %d ,loss %f&apos;%(epoch+1,train_l.mean()))</span><br></pre></td></tr></table></figure>
</li>
</ul>
</li>
<li><p><strong><code>pytorch实现</code></strong></p>
<ul>
<li><p><strong><code>1.数据读取</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">dataset = Data.TensorDataset(features,labels)</span><br><span class="line"></span><br><span class="line">data_iter = Data.DataLoader(</span><br><span class="line">	dataset = dataset,</span><br><span class="line">	batch_size = batch_size,</span><br><span class="line">	shuffle =True,</span><br><span class="line">	num_workers =2,</span><br><span class="line">	)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>2.定义模型</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">##多层网络生成法</span><br><span class="line">net = nn.Sequential(</span><br><span class="line">   nn.Linear(num_inputs, 1)</span><br><span class="line">   # other layers can be added here</span><br><span class="line">   )</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>3.模型参数初始化</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">from torch.nn import init</span><br><span class="line">init.normal_(net[0].weight, mean=0.0, std=0.01)</span><br><span class="line">init.constant_(net[0].bias, val=0.0)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>4.损失函数和优化函数</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.MSELoss()</span><br><span class="line">import torch.optim as optim</span><br><span class="line">optimizer = optim.SGD(net.parameters(), lr=0.03)   # built-in random gradient descent function</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>5.模型训练</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">for epoch in range(1, num_epochs + 1):</span><br><span class="line">   for X, y in data_iter:</span><br><span class="line">       output = net(X)</span><br><span class="line">       l = loss(output, y.view(-1, 1))</span><br><span class="line">       optimizer.zero_grad() # reset gradient, equal to net.zero_grad()</span><br><span class="line">       l.backward()</span><br><span class="line">       optimizer.step()</span><br><span class="line">   print(&apos;epoch %d, loss: %f&apos; % (epoch, l.item()))</span><br></pre></td></tr></table></figure>


</li>
</ul>
</li>
</ul>
<h2 id="softmax"><a href="#softmax" class="headerlink" title="softmax"></a>softmax</h2> <figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">是一个单层神经网络，输出时通过softmax operator将输出值变换成值为正且和为1的概率分布，把预测概率最大的类别作为输出类别</span><br><span class="line">损失函数：交叉熵损失函数，关心对正确类别的预测概率，因为只要其值足够大，就可以确保分类结果正确。（平方损失则过于严格）</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong><code>从零开始的实现核心代码</code></strong></p>
<ul>
<li><p><strong><code>1.参数初始化</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">W = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_outputs)), dtype=torch.float)</span><br><span class="line">b = torch.zeros(num_outputs, dtype=torch.float)</span><br><span class="line">W.requires_grad_(requires_grad=True)</span><br><span class="line">b.requires_grad_(requires_grad=True)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>2.定义模型</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">def softmax(X):</span><br><span class="line">	X_exp = X.exp()</span><br><span class="line">	partition = X_exp.sum(dim=1,keepdim=True)</span><br><span class="line">	return X_exp/partition</span><br><span class="line">def net(X):</span><br><span class="line">	return softmax(torch.mm(X.view((-1, num_inputs)), W) + b)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>3.损失函数和准确率</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">def cross_entry(y_hat,y):</span><br><span class="line">	return - torch.log(y_hat.gather(1,y_view(-1, 1)))</span><br><span class="line">def accuracy(y_hat, y):</span><br><span class="line">	return (y_hat.argmax(dim=1) == y).float().mean().item()</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>pytorch实现</code></strong></p>
</li>
<li><p><strong><code>1.定义模型</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">       # FlattenLayer(),</span><br><span class="line">       # nn.Linear(num_inputs, num_outputs)</span><br><span class="line">       OrderedDict([</span><br><span class="line">         (&apos;flatten&apos;, FlattenLayer()),</span><br><span class="line">         (&apos;linear&apos;, nn.Linear(num_inputs, num_outputs))])</span><br><span class="line">       )</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>2.参数初始化</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">init.normal_(net.linear.weight, mean=0, std=0.01)</span><br><span class="line">init.constant_(net.linear.bias, val=0)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>3.损失函数和优化函数</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=0.1)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<h2 id="感知机"><a href="#感知机" class="headerlink" title="感知机"></a>感知机</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"> 多层感知机就是含有至少一个隐藏层的由全连接层组成的神经网络，且每个隐藏层的输出通过激活函数进行变换。</span><br><span class="line"> 关于激活函数的选择：</span><br><span class="line">ReLu函数是一个通用的激活函数，目前在大多数情况下使用。但是，ReLU函数只能在隐藏层中使用。</span><br><span class="line">用于分类器时，sigmoid函数及其组合通常效果更好。由于梯度消失问题，有时要避免使用sigmoid和tanh函数。</span><br><span class="line">在神经网络层数较多的时候，最好使用ReLu函数，ReLu函数比较简单计算量少，而sigmoid和tanh函数计算量大很多。</span><br><span class="line">在选择激活函数的时候可以先选用ReLu函数如果效果不理想可以尝试其他激活函数。</span><br></pre></td></tr></table></figure>

<ul>
<li><p><strong><code>从零开始的实现核心代码</code></strong></p>
</li>
<li><p><strong><code>1.参数初始化</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">W1 = torch.tensor(np.random.normal(0, 0.01, (num_inputs, num_hiddens)), dtype=torch.float) ##隐藏层参数</span><br><span class="line">b1 = torch.zeros(num_hiddens, dtype=torch.float)</span><br><span class="line">W2 = torch.tensor(np.random.normal(0, 0.01, (num_hiddens, num_outputs)), dtype=torch.float) ##输出层参数</span><br><span class="line">b2 = torch.zeros(num_outputs, dtype=torch.float)</span><br><span class="line"></span><br><span class="line">params = [W1, b1, W2, b2]</span><br><span class="line">for param in params:</span><br><span class="line">	param.requires_grad_(requires_grad=True)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>2.激活函数与网络、损失函数</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">def relu(X):</span><br><span class="line">	return torch.max(input=X, other=torch.tensor(0.0))</span><br><span class="line">	</span><br><span class="line">def net(X):</span><br><span class="line">	X = X.view((-1, num_inputs))</span><br><span class="line">	H = relu(torch.matmul(X, W1) + b1)</span><br><span class="line">	return torch.matmul(H, W2) + b2</span><br><span class="line">	</span><br><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>pytorch实现</code></strong></p>
</li>
<li><p><strong><code>1.参数初始化和模型</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">net = nn.Sequential(</span><br><span class="line">       d2l.FlattenLayer(),</span><br><span class="line">       nn.Linear(num_inputs, num_hiddens),</span><br><span class="line">       nn.ReLU(),</span><br><span class="line">       nn.Linear(num_hiddens, num_outputs), </span><br><span class="line">       )</span><br><span class="line">   </span><br><span class="line">for params in net.parameters():</span><br><span class="line">	init.normal_(params, mean=0, std=0.01)</span><br></pre></td></tr></table></figure>
</li>
<li><p><strong><code>2.损失与优化</code></strong></p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">loss = torch.nn.CrossEntropyLoss()</span><br><span class="line">optimizer = torch.optim.SGD(net.parameters(), lr=0.5)</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h2 id="文本预处理"><a href="#文本预处理" class="headerlink" title="文本预处理"></a>文本预处理</h2><p>文本是一类序列数据，一篇文章可以看作是字符或单词的序列，本节将介绍文本数据的常见预处理步骤，预处理通常包括四个步骤：</p>
<p>1.读入文本<br>2.分词<br>3.建立字典，将每个词映射到一个唯一的索引（index）<br>4.将文本从词的序列转换为索引的序列，方便输入模型</p>
<ul>
<li><p><strong><code>文本预处理pytorch</code></strong></p>
<ul>
<li><strong><code>1.分词</code></strong><br>将一个句子划分成若干个词（token），转换为一个词的序列<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">def tokensize(sentences,token=&apos;word&apos;):</span><br><span class="line">	#token:做哪一个级别的分词</span><br><span class="line">	if token == &apos;word&apos;:</span><br><span class="line">		return [sentence.split(&apos; &apos;) for sentence in sentences]</span><br><span class="line">	elif token == &apos;char&apos;:</span><br><span class="line">		return [list(sentence) for sentence in sentences]</span><br><span class="line">	else:</span><br><span class="line">		print(&apos;ERROR: unkown token type&apos; + token)</span><br></pre></td></tr></table></figure>

</li>
</ul>
<p>前面介绍的分词方式非常简单，它至少有以下几个缺点:</p>
<ol>
<li>标点符号通常可以提供语义信息，但是我们的方法直接将其丢弃了</li>
<li>类似“shouldn’t”, “doesn’t”这样的词会被错误地处理</li>
<li>类似”Mr.”, “Dr.”这样的词会被错误地处理</li>
</ol>
<p>我们可以通过引入更复杂的规则来解决这些问题，但是事实上，有一些现有的工具可以很好地进行分词，如：<a href="https://spacy.io/" target="_blank" rel="noopener">spaCy</a>和<a href="https://www.nltk.org/" target="_blank" rel="noopener">NLTK</a>。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">text = &quot;Mr. Chen doesn&apos;t agree with my suggestion.&quot;</span><br><span class="line">import spacy</span><br><span class="line">nlp = spacy.load(&apos;en_core_web_sm&apos;)</span><br><span class="line">doc = nlp(text)</span><br><span class="line">print([token.text for token in doc])</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">[&apos;Mr.&apos;, &apos;Chen&apos;, &apos;does&apos;, &quot;n&apos;t&quot;, &apos;agree&apos;, &apos;with&apos;, &apos;my&apos;, &apos;suggestion&apos;, &apos;.&apos;]</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line"></span><br><span class="line">from nltk.tokenize import word_tokenize</span><br><span class="line">from nltk import data</span><br><span class="line">data.path.append(&apos;/home/kesci/input/nltk_data3784/nltk_data&apos;)</span><br><span class="line">print(word_tokenize(text))</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">[&apos;Mr.&apos;, &apos;Chen&apos;, &apos;does&apos;, &quot;n&apos;t&quot;, &apos;agree&apos;, &apos;with&apos;, &apos;my&apos;, &apos;suggestion&apos;, &apos;.&apos;]</span><br><span class="line">&apos;&apos;&apos;</span><br><span class="line">```		</span><br><span class="line">	</span><br><span class="line">- **`2.建立字典`**</span><br><span class="line">为了方便模型处理，我们需要将字符串转换为数字。因此我们需要先构建一个字典（vocabulary），将每个词映射到一个唯一的索引编号。</span><br></pre></td></tr></table></figure>

<p>class Vocab(object):<br>##去重、删除部分、添加特殊token、映射</p>
<pre><code>def __init__(self, tokens, min_freq=0, use_special_tokens=False):
    ##min_freq阈值，小于它忽略掉
    # :tokens为语料库上分词后所有的词 
    counter = count_corpus(tokens)   ##词频字典&lt;key,value&gt; &lt;词，词频&gt;
    self.token_freqs = list(counter.items()) ##拿出2元组，构造一个列表
    self.idx_to_token = [] ##记录需要维护的词

    if use_special_tokens:
        # padding, begin of sentence, end of sentence, unknown
        self.pad, self.bos, self.eos, self.unk = (0, 1, 2, 3)
        self.idx_to_token += [&apos;&lt;pad&gt;&apos;, &apos;&lt;bos&gt;&apos;, &apos;&lt;eos&gt;&apos;, &apos;&lt;unk&gt;&apos;] 
        ###pad短句补长所用,bos、eos标志开始结束，eos，unk新事物未出现在语料库--未登录词
    else:
        self.unk = 0
        self.idx_to_token += [&apos;&lt;unk&gt;&apos;]
    self.idx_to_token += [token for token, freq in self.token_freqs
                    if freq &gt;= min_freq and token not in self.idx_to_token]
    self.token_to_idx = dict()##从词到索引号的映射
    for idx, token in enumerate(self.idx_to_token):
        self.token_to_idx[token] = idx

def __len__(self):  ###字典大小
    return len(self.idx_to_token)

def __getitem__(self, tokens):  ###
    if not isinstance(tokens, (list, tuple)):
        return self.token_to_idx.get(tokens, self.unk)##直接从字典中寻找
    return [self.__getitem__(token) for token in tokens]

def to_tokens(self, indices):  ###给定索引，返回对应的词
    if not isinstance(indices, (list, tuple)):
        return self.idx_to_token[indices]
    return [self.idx_to_token[index] for index in indices]</code></pre><p>##统计词频<br>def count_corpus(sentences):</p>
<pre><code>tokens = [tk for st in sentences for tk in st]
return collections.Counter(tokens)  # 返回一个字典，记录每个词的出现次数</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- **`3.将词转为索引`**</span><br><span class="line">使用字典，我们可以将原文本中的句子从单词序列转换为索引序列</span><br></pre></td></tr></table></figure>

<p>for i in range(8, 10):<br>print(‘words:’, tokens[i])<br>print(‘indices:’, vocab[tokens[i]])<br>‘’’<br>words: [‘the’, ‘time’, ‘traveller’, ‘for’, ‘so’, ‘it’, ‘will’, ‘be’, ‘convenient’, ‘to’, ‘speak’, ‘of’, ‘him’, ‘’]<br>indices: [1, 2, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 0]<br>words: [‘was’, ‘expounding’, ‘a’, ‘recondite’, ‘matter’, ‘to’, ‘us’, ‘his’, ‘grey’, ‘eyes’, ‘shone’, ‘and’]<br>indices: [20, 21, 22, 23, 24, 16, 25, 26, 27, 28, 29, 30]<br>‘’’</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br></pre></td><td class="code"><pre><span class="line">## 语言模型</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">一段自然语言文本可以看作是一个离散时间序列，给定一个长度为$T$的词的序列$w_1, w_2, \ldots, w_T$，</span><br><span class="line">语言模型的目标就是评估该序列是否合理，即计算该序列的概率：</span><br><span class="line">$$</span><br><span class="line">P(w_1, w_2, \ldots, w_T).</span><br><span class="line">$$</span><br><span class="line">假设序列$w_1, w_2, \ldots, w_T$中的每个词是依次生成的，我们有</span><br><span class="line">$$</span><br><span class="line">\begin&#123;align*&#125;</span><br><span class="line">P(w_1, w_2, \ldots, w_T)</span><br><span class="line">&amp;= \prod_&#123;t=1&#125;^T P(w_t \mid w_1, \ldots, w_&#123;t-1&#125;)\\</span><br><span class="line">&amp;= P(w_1)P(w_2 \mid w_1) \cdots P(w_T \mid w_1w_2\cdots w_&#123;T-1&#125;)</span><br><span class="line">\end&#123;align*&#125;</span><br><span class="line">$$</span><br><span class="line">例如，一段含有4个词的文本序列的概率</span><br><span class="line">$$</span><br><span class="line">P(w_1, w_2, w_3, w_4) =  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_1, w_2, w_3).</span><br><span class="line">$$</span><br><span class="line">语言模型的参数就是词的概率以及给定前几个词情况下的条件概率。设训练数据集为一个大型文本语料库，如维基百科的所有条目，词的概率可以通过该词在训练数据集中的相对词频来计算，例如，$w_1$的概率可以计算为：</span><br><span class="line">$$</span><br><span class="line">\hat P(w_1) = \frac&#123;n(w_1)&#125;&#123;n&#125;</span><br><span class="line">$$</span><br><span class="line">其中$n(w_1)$为语料库中以$w_1$作为第一个词的文本的数量，$n$为语料库中文本的总数量。</span><br><span class="line">类似的，给定$w_1$情况下，$w_2$的条件概率可以计算为：</span><br><span class="line">$$</span><br><span class="line">\hat P(w_2 \mid w_1) = \frac&#123;n(w_1, w_2)&#125;&#123;n(w_1)&#125;</span><br><span class="line">$$</span><br><span class="line">其中$n(w_1, w_2)$为语料库中以$w_1$作为第一个词，$w_2$作为第二个词的文本的数量。</span><br><span class="line"></span><br><span class="line">  - **`1.n元语法`**</span><br><span class="line">       序列长度增加，计算和存储多个词共同出现的概率的复杂度会呈指数级增加。$n$元语法通过马尔可夫假设简化模型，马尔科夫假设是指一个词的出现只与前面$n$个词相关，即$n$阶马尔可夫链（Markov chain of order $n$），如果$n=1$，那么有$P(w_3 \mid w_1, w_2) = P(w_3 \mid w_2)$。基于$n-1$阶马尔可夫链，我们可以将语言模型改写为</span><br><span class="line">	$$</span><br><span class="line">	P(w_1, w_2, \ldots, w_T) = \prod_&#123;t=1&#125;^T P(w_t \mid w_&#123;t-(n-1)&#125;, \ldots, w_&#123;t-1&#125;) .</span><br><span class="line">	$$</span><br><span class="line">	以上也叫$n$元语法（$n$-grams），它是基于$n - 1$阶马尔可夫链的概率语言模型。例如，当$n=2$时，含有4个词的文本序列的概率就可以改写为：</span><br><span class="line">	$$</span><br><span class="line">	\begin&#123;align*&#125;</span><br><span class="line">	P(w_1, w_2, w_3, w_4)</span><br><span class="line">	&amp;= P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_1, w_2, w_3)\\</span><br><span class="line">	&amp;= P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_2) P(w_4 \mid w_3)</span><br><span class="line">	\end&#123;align*&#125;</span><br><span class="line">	$$</span><br><span class="line">	当$n$分别为1、2和3时，我们将其分别称作一元语法（unigram）、二元语法（bigram）和三元语法（trigram）。例如，长度为4的序列$w_1, w_2, w_3, w_4$在一元语法、二元语法和三元语法中的概率分别为</span><br><span class="line">	$$</span><br><span class="line">	\begin&#123;aligned&#125;</span><br><span class="line">	P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2) P(w_3) P(w_4) ,\\</span><br><span class="line">	P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_2) P(w_4 \mid w_3) ,\\</span><br><span class="line">	P(w_1, w_2, w_3, w_4) &amp;=  P(w_1) P(w_2 \mid w_1) P(w_3 \mid w_1, w_2) P(w_4 \mid w_2, w_3) .</span><br><span class="line">	\end&#123;aligned&#125;</span><br><span class="line">	$$</span><br><span class="line">	当$n$较小时，$n$元语法往往并不准确。例如，在一元语法中，由三个词组成的句子“你走先”和“你先走”的概率是一样的。然而，当$n$较大时，$n$元语法需要计算并存储大量的词频和多词相邻频率。</span><br><span class="line">	</span><br><span class="line">	**缺点：1.参数空间大、计算开销大，2.数据稀疏，齐夫定律：单词的词频与单词的排名成反比**</span><br><span class="line">	</span><br><span class="line">  - **`2.语言模型pytorch小例（中文）`**</span><br><span class="line">    - **`建立字符索引`**</span><br></pre></td></tr></table></figure>

<p>###读取数据<br>def load_data_jay_lyrics():</p>
<pre><code>with open(&apos;/home/kesci/input/jaychou_lyrics4703/jaychou_lyrics.txt&apos;) as f:
    corpus_chars = f.read()
corpus_chars = corpus_chars.replace(&apos;\n&apos;, &apos; &apos;).replace(&apos;\r&apos;, &apos; &apos;)
corpus_chars = corpus_chars[0:10000]
idx_to_char = list(set(corpus_chars))   # 去重，得到索引到字符的映射
char_to_idx = dict([(char, i) for i, char in enumerate(idx_to_char)]) # 字符到索引的映射
vocab_size = len(char_to_idx)
corpus_indices = [char_to_idx[char] for char in corpus_chars] # 将每个字符转化为索引，得到一个索引的序列
return corpus_indices, char_to_idx, idx_to_char, vocab_size</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">- **`3.时序数据采样`**</span><br><span class="line">	在训练中我们需要每次随机读取小批量样本和标签。与之前章节的实验数据不同的是，时序数据的一个样本通常包含连续的字符。假设时间步数为5，样本序列为5个字符，即“想”“要”“有”“直”“升”。</span><br><span class="line">	**该样本的标签序列为这些字符分别在训练集中的下一个字符**，即“要”“有”“直”“升”“机”，即$X$=“想要有直升”，$Y$=“要有直升机”。</span><br><span class="line">	</span><br><span class="line">	现在我们考虑序列“想要有直升机，想要和你飞到宇宙去”，如果时间步数为5，有以下可能的样本和标签：</span><br><span class="line">	* $X$：“想要有直升”，$Y$：“要有直升机”</span><br><span class="line">	* $X$：“要有直升机”，$Y$：“有直升机，”</span><br><span class="line">	* $X$：“有直升机，”，$Y$：“直升机，想”</span><br><span class="line">	* ...</span><br><span class="line">	* $X$：“要和你飞到”，$Y$：“和你飞到宇”</span><br><span class="line">	* $X$：“和你飞到宇”，$Y$：“你飞到宇宙”</span><br><span class="line">	* $X$：“你飞到宇宙”，$Y$：“飞到宇宙去”</span><br><span class="line"></span><br><span class="line">	可以看到，如果序列的长度为$T$，时间步数为$n$，那么一共有$T-n$个合法的样本，但是这些样本有大量的重合，我们通常采用更加高效的采样方式。我们有两种方式对时序数据进行采样，分别是随机采样和相邻采样。</span><br><span class="line"></span><br><span class="line">	- **`随机采样`**</span><br><span class="line">		每次从数据里随机采样一个小批量。其中批量大小`batch_size`是每个小批量的样本数，`num_steps`是每个样本所包含的时间步数。</span><br><span class="line">		在随机采样中，每个样本是原始序列上任意截取的一段序列，相邻的两个随机小批量在原始序列上的位置不一定相毗邻。</span><br></pre></td></tr></table></figure>

<pre><code>import torch
import random
def data_iter_random(corpus_indices, batch_size, num_steps, device=None):
    # 减1是因为对于长度为n的序列，X最多只有包含其中的前n - 1个字符
    num_examples = (len(corpus_indices) - 1) // num_steps  # 下取整，得到不重叠情况下的样本个数
    # 每个样本的第一个字符在corpus_indices中的下标
    example_indices = [i * num_steps for i in range(num_examples)]  
    random.shuffle(example_indices)

    def _data(i):
        # 返回从i开始的长为num_steps的序列
        return corpus_indices[i: i + num_steps]
    if device is None:
        device = torch.device(&apos;cuda&apos; if torch.cuda.is_available() else &apos;cpu&apos;)

    for i in range(0, num_examples, batch_size):
        # 每次选出batch_size个随机样本
        batch_indices = example_indices[i: i + batch_size]  # 当前batch的各个样本的首字符的下标
        X = [_data(j) for j in batch_indices]
        Y = [_data(j + 1) for j in batch_indices]
        yield torch.tensor(X, device=device), torch.tensor(Y, device=device)
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">	</span><br><span class="line">- **`随机采样`**</span><br><span class="line">	在相邻采样中，相邻的两个随机小批量在原始序列上的位置相毗邻</span><br></pre></td></tr></table></figure>

def data_iter_consecutive(corpus_indices, batch_size, num_steps, device=None):
    if device is None:
        device = torch.device(&apos;cuda&apos; if torch.cuda.is_available() else &apos;cpu&apos;)
    corpus_len = len(corpus_indices) // batch_size * batch_size  # 保留下来的序列的长度
    corpus_indices = corpus_indices[: corpus_len]  # 仅保留前corpus_len个字符
    indices = torch.tensor(corpus_indices, device=device)
    indices = indices.view(batch_size, -1)  # resize成(batch_size, )
    batch_num = (indices.shape[1] - 1) // num_steps
    for i in range(batch_num):
        i = i * num_steps
        X = indices[:, i: i + num_steps]
        Y = indices[:, i + 1: i + num_steps + 1]
        yield X, Y
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">		</span><br><span class="line">## 循环神经网络基础</span><br><span class="line">循环神经网络是基于当前的输入与过去的输入序列，预测序列的下一个字符。循环神经网络引入一个隐藏变量$H$，用$H_&#123;t&#125;$表示$H$在时间步$t$的值。$H_&#123;t&#125;$的计算基于$X_&#123;t&#125;$和$H_&#123;t-1&#125;$，可以认为$H_&#123;t&#125;$记录了到当前字符为止的序列信息，利用$H_&#123;t&#125;$对序列的下一个字符进行预测。</span><br><span class="line">![Image Name](https://cdn.kesci.com/upload/image/q5jkm0v44i.png?imageView2/0/w/640/h/640)</span><br><span class="line">  - **`结构`**</span><br><span class="line">	假设$\boldsymbol&#123;X&#125;_t \in \mathbb&#123;R&#125;^&#123;n \times d&#125;$是时间步$t$的小批量输入，$\boldsymbol&#123;H&#125;_t  \in \mathbb&#123;R&#125;^&#123;n \times h&#125;$是该时间步的隐藏变量，则：</span><br><span class="line">	$$</span><br><span class="line">	\boldsymbol&#123;H&#125;_t = \phi(\boldsymbol&#123;X&#125;_t \boldsymbol&#123;W&#125;_&#123;xh&#125; + \boldsymbol&#123;H&#125;_&#123;t-1&#125; \boldsymbol&#123;W&#125;_&#123;hh&#125;  + \boldsymbol&#123;b&#125;_h).</span><br><span class="line">	$$</span><br><span class="line">	其中，$\boldsymbol&#123;W&#125;_&#123;xh&#125; \in \mathbb&#123;R&#125;^&#123;d \times h&#125;$，$\boldsymbol&#123;W&#125;_&#123;hh&#125; \in \mathbb&#123;R&#125;^&#123;h \times h&#125;$，$\boldsymbol&#123;b&#125;_&#123;h&#125; \in \mathbb&#123;R&#125;^&#123;1 \times h&#125;$，$\phi$函数是非线性激活函数。由于引入了$\boldsymbol&#123;H&#125;_&#123;t-1&#125; \boldsymbol&#123;W&#125;_&#123;hh&#125;$，$H_&#123;t&#125;$能够捕捉截至当前时间步的序列的历史信息，就像是神经网络当前时间步的状态或记忆一样。由于$H_&#123;t&#125;$的计算基于$H_&#123;t-1&#125;$，上式的计算是循环的，使用循环计算的网络即循环神经网络（recurrent neural network）。</span><br><span class="line">	在时间步$t$，输出层的输出为：</span><br><span class="line">	$$</span><br><span class="line">	\boldsymbol&#123;O&#125;_t = \boldsymbol&#123;H&#125;_t \boldsymbol&#123;W&#125;_&#123;hq&#125; + \boldsymbol&#123;b&#125;_q.</span><br><span class="line">	$$</span><br><span class="line">	其中$\boldsymbol&#123;W&#125;_&#123;hq&#125; \in \mathbb&#123;R&#125;^&#123;h \times q&#125;$，$\boldsymbol&#123;b&#125;_q \in \mathbb&#123;R&#125;^&#123;1 \times q&#125;$。</span><br><span class="line">	</span><br><span class="line">  - **`从零开始实现循环神经网络`**</span><br><span class="line">	- **`one-hot`**</span><br><span class="line">		需要将字符表示成向量，这里采用one-hot向量。假设词典大小是$N$，每次字符对应一个从$0$到$N-1$的唯一的索引，则该字符的向量是一个长度为$N$的向量，若字符的索引是$i$，则该向量的第$i$个位置为$1$，其他位置为$0$。下面分别展示了索引为0和2的one-hot向量，向量长度等于词典大小。</span><br></pre></td></tr></table></figure></code></pre><p>def one_hot(x, n_class, dtype=torch.float32):</p>
<pre><code>##n_class :字典的大小
result = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)  # shape: (n, n_class)
result.scatter_(1, x.long().view(-1, 1), 1)  # result[i, x[i, 0]] = 1，将每一行的x[i,0]改写为1
return result</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">我们每次采样的小批量的形状是（批量大小, 时间步数）。下面的函数将这样的小批量变换成数个形状为（批量大小, 词典大小）的矩阵，矩阵个数等于时间步数。也就是说，时间步$t$的输入为$\boldsymbol&#123;X&#125;_t \in \mathbb&#123;R&#125;^&#123;n \times d&#125;$，其中$n$为批量大小，$d$为词向量大小，即one-hot向量长度（词典大小）。</span><br></pre></td></tr></table></figure>

<p>def to_onehot(X, n_class):</p>
<pre><code>return [one_hot(X[:, i], n_class) for i in range(X.shape[1])]</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">- **`初始化模型参数`**</span><br></pre></td></tr></table></figure>

<p>num_inputs, num_hiddens, num_outputs = vocab_size, 256, vocab_size</p>
<h1 id="num-inputs-d"><a href="#num-inputs-d" class="headerlink" title="num_inputs: d"></a>num_inputs: d</h1><h1 id="num-hiddens-h-隐藏单元的个数是超参数"><a href="#num-hiddens-h-隐藏单元的个数是超参数" class="headerlink" title="num_hiddens: h, 隐藏单元的个数是超参数"></a>num_hiddens: h, 隐藏单元的个数是超参数</h1><h1 id="num-outputs-q"><a href="#num-outputs-q" class="headerlink" title="num_outputs: q"></a>num_outputs: q</h1><p>def get_params():</p>
<pre><code>def _one(shape):
    param = torch.zeros(shape, device=device, dtype=torch.float32)
    nn.init.normal_(param, 0, 0.01)
    return torch.nn.Parameter(param)

# 隐藏层参数
W_xh = _one((num_inputs, num_hiddens))
W_hh = _one((num_hiddens, num_hiddens))
b_h = torch.nn.Parameter(torch.zeros(num_hiddens, device=device))
# 输出层参数
W_hq = _one((num_hiddens, num_outputs))
b_q = torch.nn.Parameter(torch.zeros(num_outputs, device=device))
return (W_xh, W_hh, b_h, W_hq, b_q)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">- **`定义模型`**</span><br><span class="line">完成前向计算</span><br><span class="line">函数`rnn`用循环的方式依次完成循环神经网络每个时间步的计算</span><br></pre></td></tr></table></figure>

<p>def rnn(inputs, state, params):</p>
<pre><code># inputs和outputs皆为num_steps个形状为(batch_size, vocab_size)的矩阵
W_xh, W_hh, b_h, W_hq, b_q = params
H, = state ##存储状态，RNN只有一个，但是LSTM不止一个，为了复用，定义为元组
outputs = []##各个时间步的输出
for X in inputs:
    H = torch.tanh(torch.matmul(X, W_xh) + torch.matmul(H, W_hh) + b_h) ##更新
    Y = torch.matmul(H, W_hq) + b_q
    outputs.append(Y)
return outputs, (H,)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">函数init_rnn_state初始化隐藏变量的初始状态，这里的返回值是一个元组。</span><br></pre></td></tr></table></figure>

<p>def init_rnn_state(batch_size, num_hiddens, device):</p>
<pre><code>return (torch.zeros((batch_size, num_hiddens), device=device), )</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">- **`剪裁梯度`**</span><br><span class="line">循环神经网络中较容易出现梯度衰减或梯度爆炸，这会导致网络几乎无法训练。裁剪梯度（clip gradient）是一种应对梯度爆炸的方法。</span><br><span class="line">假设我们把所有模型参数的梯度拼接成一个向量 $\boldsymbol&#123;g&#125;$，并设裁剪的阈值是$\theta$。裁剪后的梯度的$L_2$范数不超过$\theta$</span><br><span class="line">$$</span><br><span class="line"> \min\left(\frac&#123;\theta&#125;&#123;\|\boldsymbol&#123;g&#125;\|&#125;, 1\right)\boldsymbol&#123;g&#125;</span><br><span class="line">$$</span><br></pre></td></tr></table></figure>

<p>def grad_clipping(params, theta, device):</p>
<pre><code>norm = torch.tensor([0.0], device=device)
for param in params:
    norm += (param.grad.data ** 2).sum()
norm = norm.sqrt().item() ##||g||
if norm &gt; theta:
    for param in params:
        param.grad.data *= (theta / norm)</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">- **`定义预测函数`**</span><br><span class="line">	以下函数基于前缀`prefix`（含有数个字符的字符串）来预测接下来的`num_chars`个字符。我们将循环神经单元`rnn`设置成了函数参数，这样在后面小节介绍其他循环神经网络时能重复使用这个函数。</span><br></pre></td></tr></table></figure>

<p>def predict_rnn(prefix, num_chars, rnn, params, init_rnn_state,</p>
<pre><code>            num_hiddens, vocab_size, device, idx_to_char, char_to_idx):
state = init_rnn_state(1, num_hiddens, device)
output = [char_to_idx[prefix[0]]]   # output记录prefix加上预测的num_chars个字符
for t in range(num_chars + len(prefix) - 1):
    # 将上一时间步的输出作为当前时间步的输入
    X = to_onehot(torch.tensor([[output[-1]]], device=device), vocab_size)
    # 计算输出和更新隐藏状态
    (Y, state) = rnn(X, state, params)
    # 下一个时间步的输入是prefix里的字符或者当前的最佳预测字符
    if t &lt; len(prefix) - 1:
        output.append(char_to_idx[prefix[t + 1]])##前len(prefix)只需添加
    else:
        output.append(Y[0].argmax(dim=1).item()) ##后面的，通过预测得到
return &apos;&apos;.join([idx_to_char[i] for i in output])</code></pre><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">- **`困惑度`**</span><br><span class="line"></span><br><span class="line">我们通常使用困惑度（perplexity）来评价语言模型的好坏。“softmax回归”中交叉熵损失函数的定义。困惑度是对交叉熵损失函数做指数运算后得到的值。特别地，</span><br><span class="line"></span><br><span class="line">* 最佳情况下，模型总是把标签类别的概率预测为1，此时困惑度为1；</span><br><span class="line">* 最坏情况下，模型总是把标签类别的概率预测为0，此时困惑度为正无穷；</span><br><span class="line">* 基线情况下，模型总是预测所有类别的概率都相同，此时困惑度为类别个数。</span><br><span class="line"></span><br><span class="line">显然，任何一个有效模型的困惑度必须小于类别个数。在本例中，困惑度必须小于词典大小`vocab_size`。</span><br><span class="line"></span><br><span class="line">- **`定义模型训练函数`**</span><br><span class="line">跟之前模型训练函数相比，这里的模型训练函数有以下几点不同：</span><br><span class="line">1. 使用困惑度评价模型。</span><br><span class="line">2. 在迭代模型参数前裁剪梯度。</span><br><span class="line">3. 对时序数据采用不同采样方法将导致隐藏状态初始化的不同。</span><br></pre></td></tr></table></figure>

<p>def train_and_predict_rnn(rnn, get_params, init_rnn_state, num_hiddens,</p>
<pre><code>vocab_size, device, corpus_indices, idx_to_char,
char_to_idx, is_random_iter, num_epochs, num_steps,
lr, clipping_theta, batch_size, pred_period,
pred_len, prefixes):</code></pre><p>if is_random_iter:</p>
<pre><code>data_iter_fn = d2l.data_iter_random</code></pre><p>else:</p>
<pre><code>data_iter_fn = d2l.data_iter_consecutive</code></pre><p>params = get_params()<br>loss = nn.CrossEntropyLoss() ##损失函数</p>
<p>for epoch in range(num_epochs):</p>
<pre><code>if not is_random_iter:  # 如使用相邻采样，在epoch开始时初始化隐藏状态
    state = init_rnn_state(batch_size, num_hiddens, device)
l_sum, n, start = 0.0, 0, time.time()
data_iter = data_iter_fn(corpus_indices, batch_size, num_steps, device)
for X, Y in data_iter:
    if is_random_iter:  # 如使用随机采样，在每个小批量更新前初始化隐藏状态
        state = init_rnn_state(batch_size, num_hiddens, device)
    else:  # 否则需要使用detach函数从计算图分离隐藏状态
        for s in state:
            s.detach_()
    # inputs是num_steps个形状为(batch_size, vocab_size)的矩阵
    inputs = to_onehot(X, vocab_size)
    # outputs有num_steps个形状为(batch_size, vocab_size)的矩阵
    (outputs, state) = rnn(inputs, state, params)
    # 拼接之后形状为(num_steps * batch_size, vocab_size)
    outputs = torch.cat(outputs, dim=0)
    # Y的形状是(batch_size, num_steps)，转置后再变成形状为
    # (num_steps * batch_size,)的向量，这样跟输出的行一一对应
    y = torch.flatten(Y.T)
    # 使用交叉熵损失计算平均分类误差
    l = loss(outputs, y.long())

    # 梯度清0
    if params[0].grad is not None:
        for param in params:
            param.grad.data.zero_()
    l.backward()
    grad_clipping(params, clipping_theta, device)  # 裁剪梯度
    d2l.sgd(params, lr, 1)  # 因为误差已经取过均值，梯度不用再做平均
    l_sum += l.item() * y.shape[0]
    n += y.shape[0]

if (epoch + 1) % pred_period == 0:
    print(&apos;epoch %d, perplexity %f, time %.2f sec&apos; % (
        epoch + 1, math.exp(l_sum / n), time.time() - start))
    for prefix in prefixes:
        print(&apos; -&apos;, predict_rnn(prefix, pred_len, rnn, params, init_rnn_state,
            num_hiddens, vocab_size, device, idx_to_char, char_to_idx))</code></pre><pre><code></code></pre></li>
</ul>

      
    </div>
    <footer class="article-footer">
      <a data-url="http://yoursite.com/2020/02/13/动手学-pytorch-1/" data-id="ck6lsunfe0000ygurroxq71g6" class="article-share-link">Share</a>
      
      
    </footer>
  </div>
  
    
  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    

  
    
  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Archives</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">February 2020</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">Recent Posts</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/02/13/动手学-pytorch-1/">动手学 pytorch-task01-02</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 yui<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  <link rel="stylesheet" href="/fancybox/jquery.fancybox.css">
  <script src="/fancybox/jquery.fancybox.pack.js"></script>


<script src="/js/script.js"></script>



  </div>
</body>
</html>